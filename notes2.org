To improve the model:

experiment with different network architectures, or just change the dimensions of the LeNet layers
add regularization features like drop out or L2 regularization to make sure the network doesn't overfit the training data
tune the hyperparameters
improve the data pre-processing with steps like normalization and setting a zero mean
augment the training data by rotating or shifting images or by changing colors


ToDo:

1- Include an exploratory and visualization of the dataset.
  - Plotting traffic sign images.

DONE- Implement the basic LeNet arquitecture, train and test. Check validation accuracy.
  - Implement a normalization and grayscale images.
  - Change the number of classes, and possible preprocessing.
  - Reset the kernel and clear the output to ensure fresh start.
  - Clear the cell that loads the MNIST data and replace it with code to load the traffic sign data.
  - Delete the code that adds the images since the traffic sign images are already 32x32 pixels.
  - The traffic sign data does not come with validation set. Use the 'train test' split function in the SKLearn library to slice off a validation set from the trainig set (training of 20%).
  - Traffic signs are in color, input depth is 3.
  - The output layer has 43 classes.

After implementing the basic LeNet with epoch=10, learning rate= 0.001, batch size = 128. validation accuracy obtained = 0.872.

* Design improvements on the model and training process until get validation accuracy to 0.93.
    DONE Do some alternative preprocessing techniques (normalization, rgb to grayscale). Documenmt them.

    DONE Play with hyperparameters for training and describe how best result was obtained: batch size, #epochs, type of optimizer.
      LEARNING: 0.001, EPOCHS = 30, BATCH_SIZE=128, ADAM OPTIMZER: 0.927 (still a bit jumpy, but the last two, three epochs stayed about 0.92).
      LEARNING: 0.001, EPOCHS = 100, BATCH_SIZE=128, ADAM OPTIMZER: 0.93x
      Changing learning to 0.01: get stuch at 0.050 validation accuracy.
      Changing learning to 0.0005 and EPOCHS = 200. 0.93x accuracy, did not improve too much.
      LEARNING: 0.001, EPOCHS = 100, BATCH_SIZE=256, ADAM OPTIMZER: 0.92 (increasing the batch size did not help)
      LEARNING: 0.001, EPOCHS = 300, BATCH_SIZE= 64, ADAM OPTIMIZER: seems best performance. validation accuracy 0.94, but test is 0.92x.

      After playing with the hyperparameters, seems that learning 0.001, with epochs= 300, smaller batch seems result in a better performance, validation accuracy of 0.94.
      Now would be time for increasing the number of samples per label and augmenting the training set.

  * Number of examples per label. Increment the number of examples.
  * Generate fake data.
  * Augmenting the training set might help improve model performance. Common data augmentation techniques include rotation, translation, zoom, flips, and/or color perturbation. These techniques can be used individually or combined.
      - For translating the images, we can check "coords" which are the bounding box of the traffic sign.

  - Think about over or underfitting and regularization techniques.
  - Describe the characteristics of the arquitecture, type of model used, size of each layer.
  - Change the optimizer (Adam optimzer, stochastic gradient descent but better?). Try different optimizers.

  - Issue with rotating images: the data imbalance issue? If you are adding the rotated version of images to those classes whose number of image is lower, then you might need to use some other data augmentation techniques like histogram equalization, changing brightness, flipping, etc. https://keras.io/preprocessing/image/
  - Or you could add a few more convolution layer to increase the training accuracy I assume you are trying to improve
  - If you are improving the validation accuracy, then I might assume your training accuracy is already quite high (98,99%), then your model might be over-fitting. In that case, you will need to apply regularization, which you could use dropout layer. https://www.tensorflow.org/api_docs/python/tf/layers/dropout

4- Describe how the model solution was obtained.

5- Test with new model images.

6- Project write up.

7- Additional exercise:
  - Visualize each layer.
  - Visualize the neural network's state on test images.
  - Check other improvements of the model.
