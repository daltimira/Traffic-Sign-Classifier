To improve the model:

experiment with different network architectures, or just change the dimensions of the LeNet layers
add regularization features like drop out or L2 regularization to make sure the network doesn't overfit the training data
tune the hyperparameters
improve the data pre-processing with steps like normalization and setting a zero mean
augment the training data by rotating or shifting images or by changing colors


ToDo:

1- Include an exploratory and visualization of the dataset.
  - Plotting traffic sign images.

DONE- Implement the basic LeNet arquitecture, train and test. Check validation accuracy.
  - Implement a normalization and grayscale images.
  - Change the number of classes, and possible preprocessing.
  - Reset the kernel and clear the output to ensure fresh start.
  - Clear the cell that loads the MNIST data and replace it with code to load the traffic sign data.
  - Delete the code that adds the images since the traffic sign images are already 32x32 pixels.
  - The traffic sign data does not come with validation set. Use the 'train test' split function in the SKLearn library to slice off a validation set from the trainig set (training of 20%).
  - Traffic signs are in color, input depth is 3.
  - The output layer has 43 classes.

After implementing the basic LeNet with epoch=10, learning rate= 0.001, batch size = 128. validation accuracy obtained = 0.872.

DONE Design improvements on the model and training process until get validation accuracy to 0.93.
    DONE Do some alternative preprocessing techniques (normalization, rgb to grayscale). Documenmt them.

    DONE Play with hyperparameters for training and describe how best result was obtained: batch size, #epochs, type of optimizer.
      LEARNING: 0.001, EPOCHS = 30, BATCH_SIZE=128, ADAM OPTIMZER: 0.927 (still a bit jumpy, but the last two, three epochs stayed about 0.92).
      LEARNING: 0.001, EPOCHS = 100, BATCH_SIZE=128, ADAM OPTIMZER: 0.93x
      Changing learning to 0.01: get stuch at 0.050 validation accuracy.
      Changing learning to 0.0005 and EPOCHS = 200. 0.93x accuracy, did not improve too much.
      LEARNING: 0.001, EPOCHS = 100, BATCH_SIZE=256, ADAM OPTIMZER: 0.92 (increasing the batch size did not help)
      LEARNING: 0.001, EPOCHS = 300, BATCH_SIZE= 64, ADAM OPTIMIZER: seems best performance. validation accuracy 0.94, but test is 0.92x.

      After playing with the hyperparameters, seems that learning 0.001, with epochs= 300, smaller batch seems result in a better performance, validation accuracy of 0.94.
      Now would be time for increasing the number of samples per label and augmenting the training set.

    DONE Number of examples per label. Increment the number of examples.
    DONE Generate fake data.
    DONE Augmenting the training set might help improve model performance. Common data augmentation techniques include rotation, translation, zoom, flips, and/or color perturbation. These techniques can be used individually or combined.
      - For translating the images, we can check "coords" which are the bounding box of the traffic sign.
      - https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
      - Data augmentation is one way to fight overfitting (when have few samples). However augmented images are highly correlated.


      EPOCH 30, BATCH SIZE 128, WITH AGUMENTED SAMPLES:
      Augmenting the samples, just translation, makes worse validation accuracy.
      Duplicating the training samples, makes a good training validation (0.93), and good test accuracy (0.929)
      Using ImageGeneration without parameters, I get a training validation of 0.944, but test accuracy (0.929)
      With just rotation: training validation (0.932), but validation test: 0.914.

    DONE Balance data.
      First attemp have been in augmenting the images and by doing this, balance the data. I have attempted to create 3000 samples for each class (initial training + the necessary augmented data to achieve reach 3000 samples).
      With just rotation and translation, achieved a validation accuracy of 0.856 with 30 epochs and 128 bach size. So not good.
      Balancing with 1500 images. Validation accuracy: 0.909
      Balancing with 800 images, and just shifting and zooming.
      Balancing with 600 images, epochs 300, batch 64. with augmetation (zoom range, translation and brightness): 0.938, but test is 0.915.
        Validation accuracy was never over 0.94, and the test accuracy never reached 0.93. So after these trials, decided to try the regularization techniques (dropouts).
        Did not try higher balancing with augmentation as this would increase the image correlations.




  DONE Think about over or underfitting and regularization techniques.
    - Implemented dropout regularization technique. The keep probability for the training set 0.5, and for evaluation set to 1.0.
    - I have put a dropout after each activation function in the LeNet arquitecture.
    - After running batches of 128, 30 epochs, and with data augmentation of 600: validation accuracy: 0.912, test accuracy: 0.912.
    rotation_range=25,
    zoom_range=0.3,
    horizontal_flip=False,
    vertical_flip=False,
    fill_mode='nearest',
    width_shift_range=0.2,
    height_shift_range=0.2,
    brightness_range=[0.5, 1.5]
    - Now with only zoom range and shift. validation accuracy: 0.924, test accuracy: 0.924.
    - By removing the augmentation samples, but keeping regularization: validation accuracy: 0.944, test accuracy: 0.931 !!!!!
    - By removing the last two layers of dropout, increase validation accuracy (and still without any agumentation samples): 0.928. We keep all droputs.
    - Putting back all the dropuuts, but increasing the probability of keeping activation from 0.5 to 0.7: validation accury: 0.956, test accuracy: 0.943 !!!!! wow!!!

    Regularization made really improve the model.

    It seems that keeping probability of 0.7 is a good choice.
    - Putting back 600 augmentation images. Validation accuracy: 0.949. Test accuracy: 0.945. Best so far!
    - Putting back the brightness back: validation accuracy: 0.949. Test accuracy: 0.933. Let's remove the brightness.
    - Running back with augmented images, got a validation accuracy of 0.92x.
    - Removing the augmented images: 0.958 (and test accuracy: 0.94).
    - Seems that with 100 epochs, augmented images, and batch size of 64 can be enough for a good model.
    - With 300 epochs and bactch size of 64, 0.958 (validation accuracy), and test validation (0.940)


  - Describe the characteristics of the arquitecture, type of model used, size of each layer.
  - Change the optimizer (Adam optimzer, stochastic gradient descent but better?). Try different optimizers.

  - Issue with rotating images: the data imbalance issue? If you are adding the rotated version of images to those classes whose number of image is lower, then you might need to use some other data augmentation techniques like histogram equalization, changing brightness, flipping, etc. https://keras.io/preprocessing/image/
  - Or you could add a few more convolution layer to increase the training accuracy I assume you are trying to improve
  - If you are improving the validation accuracy, then I might assume your training accuracy is already quite high (98,99%), then your model might be over-fitting. In that case, you will need to apply regularization, which you could use dropout layer. https://www.tensorflow.org/api_docs/python/tf/layers/dropout

DONE Clean code and make the final run with the final model.

* Test with new model images.

6- Project write up.
  - Describe how the model solution was obtained.
  - Re-read the notebook if anything else need to be cleaned.

7- Additional exercise:
  - Visualize each layer.
  - Visualize the neural network's state on test images.
  - Check other improvements of the model.



  # As we have inbalanced data we can calculate the weights so that learn more for the less frequent data?
  from sklearn.utils import class_weight
  class_weights = class_weight.compute_class_weight('balanced',
                                                   np.unique(y_train),
                                                   y_train)
  print(class_weights)
